{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yolov5-training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1__j1P0K4veULhLeQ7z8HFCOgyXdKNnkj",
      "authorship_tag": "ABX9TyMUnpv01lV20T0TTV3oLLcO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/himanshu-dutta/mb-object-detection-yolov5/blob/master/yolov5-training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVMcVLsOZdWP",
        "outputId": "2c479366-33c7-4c19-ac9b-e126f6bea251"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Feb 15 12:30:03 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qMqWr6icXgS",
        "outputId": "e19e5478-d4f1-4a73-e3f7-34bfd8e2d76a"
      },
      "source": [
        "!sudo apt-get install tree -qq > /dev/null\n",
        "!pip install git+https://github.com/cocodataset/cocoapi.git\n",
        "!git clone https://github.com/NVIDIA/apex && cd apex && pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" . --user && cd .. && rm -rf apex"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Collecting git+https://github.com/cocodataset/cocoapi.git\n",
            "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-76edicbk\n",
            "  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-76edicbk\n",
            "\u001b[31mERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 7891 (delta 0), reused 0 (delta 0), pack-reused 7885\u001b[K\n",
            "Receiving objects: 100% (7891/7891), 13.99 MiB | 27.75 MiB/s, done.\n",
            "Resolving deltas: 100% (5385/5385), done.\n",
            "/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py:283: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-qtpglywj\n",
            "Created temporary directory: /tmp/pip-req-tracker-t_7zu_9z\n",
            "Created requirements tracker '/tmp/pip-req-tracker-t_7zu_9z'\n",
            "Created temporary directory: /tmp/pip-install-fpcv8pah\n",
            "Processing /content/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-wgx38ocp\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-t_7zu_9z'\n",
            "    Running setup.py (path:/tmp/pip-req-build-wgx38ocp/setup.py) egg_info for package from file:///content/apex\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.7.0+cu101\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-req-build-wgx38ocp/pip-egg-info/apex.egg-info\n",
            "    writing /tmp/pip-req-build-wgx38ocp/pip-egg-info/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-req-build-wgx38ocp/pip-egg-info/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-req-build-wgx38ocp/pip-egg-info/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-req-build-wgx38ocp/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file '/tmp/pip-req-build-wgx38ocp/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-wgx38ocp/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-wgx38ocp has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-t_7zu_9z'\n",
            "Skipping wheel build for apex, due to binaries being disabled for it.\n",
            "Installing collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-record-v12g9x7y\n",
            "    Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-wgx38ocp/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-wgx38ocp/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-v12g9x7y/install-record.txt --single-version-externally-managed --compile --user --prefix=\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.7.0+cu101\n",
            "\n",
            "\n",
            "    /tmp/pip-req-build-wgx38ocp/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "\n",
            "    Compiling cuda extensions with\n",
            "    nvcc: NVIDIA (R) Cuda compiler driver\n",
            "    Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "    Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "    Cuda compilation tools, release 10.1, V10.1.243\n",
            "    from /usr/local/cuda/bin\n",
            "\n",
            "    running install\n",
            "    running build\n",
            "    running build_py\n",
            "    creating build\n",
            "    creating build/lib.linux-x86_64-3.6\n",
            "    creating build/lib.linux-x86_64-3.6/apex\n",
            "    copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\n",
            "    creating build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    creating build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "    copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "    creating build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    creating build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    creating build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    creating build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    creating build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib\n",
            "    copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    running build_ext\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/utils/cpp_extension.py:339: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "      warnings.warn(msg.format('we could not find ninja.'))\n",
            "    building 'apex_C' extension\n",
            "    creating build/temp.linux-x86_64-3.6\n",
            "    creating build/temp.linux-x86_64-3.6/csrc\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/flatten_unflatten.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    In file included from csrc/flatten_unflatten.cpp:2:0:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h: In member function ‘at::DeprecatedTypeProperties& torch::utils::TensorGroup::type()’:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h:36:28: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         return tensors[0].type();\n",
            "                                ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/flatten_unflatten.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'amp_C' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/amp_C_frontend.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'syncbn' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/syncbn.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'fused_layer_norm_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm(at::Tensor, c10::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:129:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_affine(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:149:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:150:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:151:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘at::Tensor layer_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:193:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:194:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:195:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:196:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:218:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:219:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:220:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:221:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:222:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:223:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'mlp_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/mlp.cpp -o build/temp.linux-x86_64-3.6/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:56:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:64:77: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n",
            "                                                                                 ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
            "                                                                        ^\n",
            "    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:67:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:150:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                            \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:113:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:119:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < inputs.size(); i++) {\n",
            "                       ~~^~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:120:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:123:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:150:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                            \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/mlp.o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "    running install_lib\n",
            "    copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> /root/.local/lib/python3.6/site-packages\n",
            "    copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> /root/.local/lib/python3.6/site-packages\n",
            "    copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> /root/.local/lib/python3.6/site-packages\n",
            "    copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> /root/.local/lib/python3.6/site-packages\n",
            "    copying build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so -> /root/.local/lib/python3.6/site-packages\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> /root/.local/lib/python3.6/site-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> /root/.local/lib/python3.6/site-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> /root/.local/lib/python3.6/site-packages/apex/fp16_utils\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> /root/.local/lib/python3.6/site-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/normalization\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/pyprof\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/nvvp.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/parse.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/db.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/kernel.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__main__.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/parse\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/randomSample.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/index_slice_join_mutate.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/prof.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/dropout.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/reduction.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/data.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/softmax.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/usage.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/activation.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/embedding.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/convert.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/conv.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/utility.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/output.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/linear.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/misc.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/optim.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/blas.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/normalization.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/recurrentCell.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pointwise.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/loss.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pooling.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__main__.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/base.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/nvmarker.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/nvtx\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/mlp/mlp.py -> /root/.local/lib/python3.6/site-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/mlp/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/mlp\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> /root/.local/lib/python3.6/site-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> /root/.local/lib/python3.6/site-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> /root/.local/lib/python3.6/site-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/RNN\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> /root/.local/lib/python3.6/site-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/__init__.py -> /root/.local/lib/python3.6/site-packages/apex\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> /root/.local/lib/python3.6/site-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> /root/.local/lib/python3.6/site-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> /root/.local/lib/python3.6/site-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> /root/.local/lib/python3.6/site-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> /root/.local/lib/python3.6/site-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> /root/.local/lib/python3.6/site-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> /root/.local/lib/python3.6/site-packages/apex/parallel\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> /root/.local/lib/python3.6/site-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> /root/.local/lib/python3.6/site-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> /root/.local/lib/python3.6/site-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_novograd.py -> /root/.local/lib/python3.6/site-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_sgd.py -> /root/.local/lib/python3.6/site-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adagrad.py -> /root/.local/lib/python3.6/site-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_lamb.py -> /root/.local/lib/python3.6/site-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> /root/.local/lib/python3.6/site-packages/apex/optimizers\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> /root/.local/lib/python3.6/site-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> /root/.local/lib/python3.6/site-packages/apex/reparameterization\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/contrib\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/softmax_xentropy.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/xentropy\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/asp.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/sparse_masklib.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/contrib\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v3.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_lamb.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_sgd.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_lamb.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v2.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fp16_optimizer.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_adam.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn_func.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n",
            "    creating /root/.local/lib/python3.6/site-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/batch_norm.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/groupbn\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/normalization/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/parse/nvvp.py to nvvp.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/parse/parse.py to parse.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/parse/db.py to db.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/parse/kernel.py to kernel.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/parse/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/parse/__main__.py to __main__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/randomSample.py to randomSample.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/prof.py to prof.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/dropout.py to dropout.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/reduction.py to reduction.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/data.py to data.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/softmax.py to softmax.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/usage.py to usage.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/activation.py to activation.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/embedding.py to embedding.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/convert.py to convert.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/conv.py to conv.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/utility.py to utility.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/output.py to output.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/linear.py to linear.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/misc.py to misc.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/optim.py to optim.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/blas.py to blas.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/normalization.py to normalization.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/pointwise.py to pointwise.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/loss.py to loss.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/pooling.py to pooling.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/__main__.py to __main__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/base.py to base.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/nvtx/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/mlp/mlp.py to mlp.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/mlp/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/RNN/cells.py to cells.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/RNN/models.py to models.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/RNN/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/parallel/LARC.py to LARC.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/parallel/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/parallel/distributed.py to distributed.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/opt.py to opt.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/utils.py to utils.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/scaler.py to scaler.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/wrap.py to wrap.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/_initialize.py to _initialize.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/frontend.py to frontend.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/compat.py to compat.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/handle.py to handle.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/__version__.py to __version__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/amp.py to amp.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/optimizers/fused_novograd.py to fused_novograd.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/xentropy/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/sparsity/asp.py to asp.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/sparsity/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers/distributed_fused_adam_v3.py to distributed_fused_adam_v3.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers/distributed_fused_adam_v2.py to distributed_fused_adam_v2.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-36.pyc\n",
            "    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/groupbn/__init__.py to __init__.cpython-36.pyc\n",
            "    running install_egg_info\n",
            "    running egg_info\n",
            "    creating apex.egg-info\n",
            "    writing apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to apex.egg-info/top_level.txt\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    Copying apex.egg-info to /root/.local/lib/python3.6/site-packages/apex-0.1-py3.6.egg-info\n",
            "    running install_scripts\n",
            "    writing list of installed files to '/tmp/pip-record-v12g9x7y/install-record.txt'\n",
            "    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n",
            "  Removing source in /tmp/pip-req-build-wgx38ocp\n",
            "Successfully installed apex-0.1\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-t_7zu_9z'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_f_b0KzcY0I",
        "outputId": "6e0563c3-79b1-4121-8845-e69dadb5b8ca"
      },
      "source": [
        "!git clone https://github.com/ultralytics/yolov5.git\n",
        "!pip install -r yolov5/requirements.txt -q"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 4809 (delta 13), reused 16 (delta 5), pack-reused 4779\u001b[K\n",
            "Receiving objects: 100% (4809/4809), 7.80 MiB | 34.45 MiB/s, done.\n",
            "Resolving deltas: 100% (3274/3274), done.\n",
            "\u001b[K     |████████████████████████████████| 645kB 12.9MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AeMgn5sc74b"
      },
      "source": [
        "# Setting up the dataset and the config files\n",
        "\n",
        "### {assuming g-drive is connected to the nb}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB6pdfHMdCBQ"
      },
      "source": [
        "!unzip -qq /content/drive/MyDrive/Projects/food-ingredients/top-10/data.zip -d /content/\n",
        "!cp  /content/drive/MyDrive/Projects/food-ingredients/top-10/food_data.yaml /content/yolov5/data/\n",
        "!cp /content/drive/MyDrive/Projects/food-ingredients/top-10/yolov5m.yaml /content/yolov5/models/"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8N6qc1E4dykU",
        "outputId": "1aed1877-f499-4aeb-db9c-6cabe5150cb0"
      },
      "source": [
        "# directory structure for the data\n",
        "!tree data -L 2"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data\n",
            "├── images\n",
            "│   ├── train\n",
            "│   └── val\n",
            "└── labels\n",
            "    ├── train\n",
            "    └── val\n",
            "\n",
            "6 directories, 0 files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4ZfNN02d57g",
        "outputId": "bd81f8eb-d4fa-4648-f128-ce55f3e92140"
      },
      "source": [
        "# yolo format for the bounding box information\n",
        "!cat \"/content/data/labels/train/curry leaves_05ec7e800b.txt\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.556225 0.492574 0.887550 0.727723\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OXfyX6DeQ3y"
      },
      "source": [
        "# Fine-tuning YOLOv5 model\n",
        "\n",
        "### {yolov5l is being used as of now}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xf-mzw-niwZC",
        "outputId": "183337ac-11a4-4afc-8b95-85ea39118934"
      },
      "source": [
        "%cd yolov5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/yolov5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WHS6-28eQix",
        "outputId": "f9ef71b5-b85d-4431-9042-60eac331e316"
      },
      "source": [
        "TRAIN_NAME = \"food_ingredients_yolov5m\"\n",
        "\n",
        "!python train.py --img 640 --batch 16 --epochs 100 \\\n",
        "  --data ./data/food_data.yaml --cfg ./models/yolov5m.yaml --weights yolov5m.pt \\\n",
        "  --name {TRAIN_NAME} --cache"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 v4.0-77-g3e560e2 torch 1.7.0+cu101 CUDA:0 (Tesla T4, 15109.75MB)\n",
            "\n",
            "Namespace(adam=False, batch_size=16, bucket='', cache_images=True, cfg='./models/yolov5m.yaml', data='./data/food_data.yaml', device='', epochs=100, evolve=False, exist_ok=False, global_rank=-1, hyp='data/hyp.scratch.yaml', image_weights=False, img_size=[640, 640], linear_lr=False, local_rank=-1, log_artifacts=False, log_imgs=16, multi_scale=False, name='food_ingredients_yolov5m', noautoanchor=False, nosave=False, notest=False, project='runs/train', quad=False, rect=False, resume=False, save_dir='runs/train/food_ingredients_yolov5m', single_cls=False, sync_bn=False, total_batch_size=16, weights='yolov5m.pt', workers=8, world_size=1)\n",
            "\u001b[34m\u001b[1mwandb: \u001b[0mInstall Weights & Biases for YOLOv5 logging with 'pip install wandb' (recommended)\n",
            "Start Tensorboard with \"tensorboard --logdir runs/train\", view at http://localhost:6006/\n",
            "2021-02-13 23:36:04.698690: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v4.0/yolov5m.pt to yolov5m.pt...\n",
            "100% 41.1M/41.1M [00:00<00:00, 123MB/s]\n",
            "\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      5280  models.common.Focus                     [3, 48, 3]                    \n",
            "  1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                \n",
            "  2                -1  1     65280  models.common.C3                        [96, 96, 2]                   \n",
            "  3                -1  1    166272  models.common.Conv                      [96, 192, 3, 2]               \n",
            "  4                -1  1    629760  models.common.C3                        [192, 192, 6]                 \n",
            "  5                -1  1    664320  models.common.Conv                      [192, 384, 3, 2]              \n",
            "  6                -1  1   2512896  models.common.C3                        [384, 384, 6]                 \n",
            "  7                -1  1   2655744  models.common.Conv                      [384, 768, 3, 2]              \n",
            "  8                -1  1   1476864  models.common.SPP                       [768, 768, [5, 9, 13]]        \n",
            "  9                -1  1   4134912  models.common.C3                        [768, 768, 2, False]          \n",
            " 10                -1  1    295680  models.common.Conv                      [768, 384, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1   1182720  models.common.C3                        [768, 384, 2, False]          \n",
            " 14                -1  1     74112  models.common.Conv                      [384, 192, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1    296448  models.common.C3                        [384, 192, 2, False]          \n",
            " 18                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1   1035264  models.common.C3                        [384, 384, 2, False]          \n",
            " 21                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   4134912  models.common.C3                        [768, 768, 2, False]          \n",
            " 24      [17, 20, 23]  1     60615  models.yolo.Detect                      [10, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [192, 384, 768]]\n",
            "Model Summary: 391 layers, 21092775 parameters, 21092775 gradients, 50.5 GFLOPS\n",
            "\n",
            "Transferred 498/506 items from yolov5m.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "Optimizer groups: 86 .bias, 86 conv.weight, 83 other\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../data/labels/train' for images and labels... 255 found, 147 missing, 0 empty, 0 corrupted: 100% 402/402 [00:00<00:00, 3893.91it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: ../data/labels/train.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.4GB): 100% 402/402 [00:02<00:00, 160.50it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../data/labels/val' for images and labels... 75 found, 20 missing, 0 empty, 0 corrupted: 100% 95/95 [00:00<00:00, 1945.48it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: ../data/labels/val.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB): 100% 95/95 [00:00<00:00, 119.76it/s]\n",
            "Plotting labels... \n",
            "\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 3.16, Best Possible Recall (BPR) = 1.0000\n",
            "Image sizes 640 train, 640 test\n",
            "Using 2 dataloader workers\n",
            "Logging results to runs/train/food_ingredients_yolov5m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "      0/99     9.32G   0.09399   0.03063   0.06207    0.1867         4       640: 100% 26/26 [00:18<00:00,  1.38it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:07<00:00,  2.50s/it]\n",
            "                 all          95         144       0.105      0.0037    0.000688    0.000124\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "      1/99     9.26G   0.07288   0.02795   0.05604    0.1569         4       640: 100% 26/26 [00:11<00:00,  2.36it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.73it/s]\n",
            "                 all          95         144      0.0257      0.0222      0.0135     0.00589\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "      2/99     9.26G   0.06544   0.02904   0.05621    0.1507         5       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.91it/s]\n",
            "                 all          95         144      0.0881      0.0556      0.0703      0.0218\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "      3/99     9.26G   0.05835   0.02759    0.0552    0.1411         2       640: 100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.90it/s]\n",
            "                 all          95         144      0.0803       0.214      0.0727      0.0306\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "      4/99     9.26G    0.0484   0.02772   0.05397    0.1301         4       640: 100% 26/26 [00:11<00:00,  2.32it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.90it/s]\n",
            "                 all          95         144      0.0883       0.327      0.0902      0.0428\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "      5/99     9.26G   0.04619   0.02688   0.05243    0.1255         4       640: 100% 26/26 [00:11<00:00,  2.31it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.94it/s]\n",
            "                 all          95         144       0.254       0.293        0.18       0.101\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "      6/99     9.26G   0.04574   0.02585   0.04654    0.1181         7       640: 100% 26/26 [00:10<00:00,  2.37it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.87it/s]\n",
            "                 all          95         144       0.101       0.454       0.174       0.104\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "      7/99     9.26G   0.04394   0.02544   0.04351    0.1129         5       640: 100% 26/26 [00:10<00:00,  2.39it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.89it/s]\n",
            "                 all          95         144       0.433       0.301       0.211      0.0982\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "      8/99     9.26G   0.04316   0.02458   0.04224      0.11         5       640: 100% 26/26 [00:10<00:00,  2.38it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.89it/s]\n",
            "                 all          95         144       0.573       0.314       0.345       0.184\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "      9/99     9.26G    0.0433   0.02384   0.03956    0.1067         8       640: 100% 26/26 [00:10<00:00,  2.38it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.88it/s]\n",
            "                 all          95         144       0.527        0.36       0.339       0.185\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     10/99     9.26G   0.03956   0.02168   0.03529   0.09653         7       640: 100% 26/26 [00:10<00:00,  2.38it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.87it/s]\n",
            "                 all          95         144       0.403       0.371       0.351       0.195\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     11/99     9.26G   0.04476   0.02123   0.03527    0.1013         6       640: 100% 26/26 [00:11<00:00,  2.36it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.82it/s]\n",
            "                 all          95         144       0.365       0.374       0.307        0.15\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     12/99     9.26G   0.04363   0.01985    0.0332   0.09668         3       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.85it/s]\n",
            "                 all          95         144       0.504       0.328       0.342       0.173\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     13/99     9.26G   0.04235   0.02104   0.03216   0.09554        14       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.86it/s]\n",
            "                 all          95         144       0.391       0.393       0.293       0.131\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     14/99     9.26G   0.04123   0.01811   0.02904   0.08839         2       640: 100% 26/26 [00:11<00:00,  2.36it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.86it/s]\n",
            "                 all          95         144       0.417       0.431       0.358       0.178\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     15/99     9.26G   0.04293   0.01847   0.02628   0.08768         7       640: 100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.87it/s]\n",
            "                 all          95         144       0.643        0.38       0.432       0.248\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     16/99     9.26G   0.04001   0.01728   0.02529   0.08258         2       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.86it/s]\n",
            "                 all          95         144        0.39       0.454       0.375       0.198\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     17/99     9.26G   0.04474   0.01777   0.02364   0.08615         9       640: 100% 26/26 [00:11<00:00,  2.33it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.90it/s]\n",
            "                 all          95         144       0.454       0.531       0.481       0.287\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     18/99     9.26G   0.04239   0.01812   0.02537   0.08589         7       640: 100% 26/26 [00:11<00:00,  2.33it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.96it/s]\n",
            "                 all          95         144       0.347       0.245       0.187       0.085\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     19/99     9.26G   0.04147   0.01811   0.02342     0.083         2       640: 100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.93it/s]\n",
            "                 all          95         144       0.219       0.605       0.385       0.193\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     20/99     9.26G   0.04357   0.01807   0.02068   0.08232         5       640: 100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.89it/s]\n",
            "                 all          95         144       0.379       0.521       0.433       0.249\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     21/99     9.26G   0.04223   0.01682   0.02035    0.0794         3       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.84it/s]\n",
            "                 all          95         144       0.348       0.644       0.488       0.302\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     22/99     9.26G   0.04489   0.01651   0.01994   0.08134         4       640: 100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.82it/s]\n",
            "                 all          95         144       0.423       0.465       0.424       0.259\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     23/99     9.26G   0.04483   0.01624   0.01961   0.08068         5       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.86it/s]\n",
            "                 all          95         144       0.492       0.625       0.537       0.307\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     24/99     9.26G   0.04467   0.01736   0.01636   0.07839        13       640: 100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.91it/s]\n",
            "                 all          95         144       0.512         0.5       0.447       0.247\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     25/99     9.26G   0.04274   0.01637   0.01548   0.07459         2       640: 100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.85it/s]\n",
            "                 all          95         144       0.469       0.599       0.536       0.335\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     26/99     9.26G   0.04301   0.01612   0.01375   0.07288         6       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.85it/s]\n",
            "                 all          95         144       0.408        0.71       0.546       0.329\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     27/99     9.26G   0.04061   0.01573   0.01412   0.07046         1       640: 100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.84it/s]\n",
            "                 all          95         144       0.363       0.588       0.464       0.243\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     28/99     9.26G   0.04217   0.01606   0.01733   0.07556         3       640: 100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.87it/s]\n",
            "                 all          95         144       0.414       0.369       0.328       0.184\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     29/99     9.26G   0.04407   0.01949   0.01819   0.08175        11       640: 100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.77it/s]\n",
            "                 all          95         144       0.387       0.489       0.365       0.194\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     30/99     9.26G   0.04088    0.0194   0.01856   0.07883         4       640: 100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.90it/s]\n",
            "                 all          95         144       0.328       0.432       0.321       0.193\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     31/99     9.26G    0.0421   0.01842   0.01697   0.07749         4       640: 100% 26/26 [00:11<00:00,  2.33it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.87it/s]\n",
            "                 all          95         144       0.613       0.546       0.571       0.256\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     32/99     9.26G   0.04485   0.01767   0.02202   0.08455         5       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.81it/s]\n",
            "                 all          95         144       0.531       0.544       0.516       0.288\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     33/99     9.26G   0.04428   0.01719   0.01772   0.07919         2       640: 100% 26/26 [00:11<00:00,  2.36it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "                 all          95         144       0.356       0.523       0.431       0.257\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     34/99     9.26G   0.03985   0.01682   0.01467   0.07134         7       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.87it/s]\n",
            "                 all          95         144       0.583       0.376       0.398        0.25\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     35/99     9.26G   0.03988   0.01667   0.01749   0.07404         6       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.84it/s]\n",
            "                 all          95         144       0.348        0.49       0.342       0.186\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     36/99     9.26G   0.04549    0.0179   0.01933   0.08271         4       640: 100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.91it/s]\n",
            "                 all          95         144       0.591       0.351       0.308       0.194\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     37/99     9.26G   0.04437   0.01924   0.02187   0.08548         4       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.89it/s]\n",
            "                 all          95         144       0.397       0.234       0.201       0.102\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     38/99     9.26G    0.0445   0.02023   0.02395   0.08867         2       640: 100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "                 all          95         144       0.479       0.572         0.5       0.285\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     39/99     9.26G   0.03999    0.0206   0.01692   0.07751         4       640: 100% 26/26 [00:11<00:00,  2.36it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.82it/s]\n",
            "                 all          95         144       0.321       0.453       0.362       0.186\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     40/99     9.26G   0.03996   0.01907   0.01596   0.07499         7       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.84it/s]\n",
            "                 all          95         144       0.462       0.475       0.453       0.257\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     41/99     9.26G   0.04089   0.01838   0.01792   0.07719         7       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.71it/s]\n",
            "                 all          95         144        0.32       0.539       0.408       0.232\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     42/99     9.26G   0.04507   0.02025   0.01633   0.08166         7       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.77it/s]\n",
            "                 all          95         144        0.38       0.524       0.403       0.238\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     43/99     9.26G   0.03834   0.01886   0.01796   0.07516         4       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.87it/s]\n",
            "                 all          95         144        0.51       0.323       0.316       0.154\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     44/99     9.26G   0.04282   0.02175   0.01951   0.08407         5       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.81it/s]\n",
            "                 all          95         144       0.351       0.399       0.299       0.163\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     45/99     9.26G   0.03927   0.02022   0.01939   0.07888         2       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.88it/s]\n",
            "                 all          95         144       0.479       0.474       0.458       0.265\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     46/99     9.26G   0.04095   0.01976   0.01645   0.07716         3       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "                 all          95         144       0.582       0.399       0.405       0.238\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     47/99     9.26G   0.04247    0.0188   0.01434    0.0756         3       640: 100% 26/26 [00:11<00:00,  2.33it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.90it/s]\n",
            "                 all          95         144       0.551       0.491       0.508       0.301\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     48/99     9.26G   0.03794   0.01832    0.0144   0.07067         5       640: 100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "                 all          95         144       0.568       0.446       0.454       0.265\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     49/99     9.26G   0.03719    0.0189   0.01299   0.06908         4       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.78it/s]\n",
            "                 all          95         144       0.583       0.593       0.571       0.362\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     50/99     9.26G   0.04025    0.0179   0.01296   0.07111         4       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.81it/s]\n",
            "                 all          95         144       0.439       0.611       0.529       0.338\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     51/99     9.26G   0.03743   0.01888   0.01307   0.06937         5       640: 100% 26/26 [00:11<00:00,  2.36it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.85it/s]\n",
            "                 all          95         144       0.559       0.591       0.541       0.364\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     52/99     9.26G   0.03647   0.01796   0.01634   0.07077         5       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.87it/s]\n",
            "                 all          95         144       0.462       0.454       0.475       0.305\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     53/99     9.26G   0.03802   0.01852   0.01839   0.07493         5       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.82it/s]\n",
            "                 all          95         144       0.514       0.574       0.531       0.338\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     54/99     9.26G   0.03804   0.01724   0.01714   0.07242         2       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.84it/s]\n",
            "                 all          95         144       0.575       0.514       0.516       0.319\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     55/99     9.26G   0.03329   0.01817   0.01287   0.06433         4       640: 100% 26/26 [00:11<00:00,  2.36it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "                 all          95         144       0.544       0.483       0.503       0.295\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     56/99     9.26G   0.03833   0.01753   0.01419   0.07006         4       640: 100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.90it/s]\n",
            "                 all          95         144       0.483       0.433       0.385       0.223\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     57/99     9.26G   0.03574   0.01619   0.01699   0.06891         6       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "                 all          95         144        0.56       0.485       0.502       0.331\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     58/99     9.26G   0.03406   0.01754   0.01349   0.06508         7       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.84it/s]\n",
            "                 all          95         144       0.613       0.543       0.574       0.386\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     59/99     9.26G   0.03506   0.01611   0.01344    0.0646         6       640: 100% 26/26 [00:11<00:00,  2.36it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.86it/s]\n",
            "                 all          95         144       0.636       0.621       0.636       0.404\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     60/99     9.26G   0.03807   0.01668   0.01275    0.0675         6       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.87it/s]\n",
            "                 all          95         144       0.537       0.561       0.533       0.358\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     61/99     9.26G   0.03518   0.01744    0.0139   0.06652         3       640: 100% 26/26 [00:11<00:00,  2.36it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.87it/s]\n",
            "                 all          95         144       0.625       0.559       0.606       0.407\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     62/99     9.26G   0.03369   0.01731   0.01161   0.06261         3       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.84it/s]\n",
            "                 all          95         144       0.591       0.521       0.535       0.363\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     63/99     9.26G   0.03402   0.01748   0.01255   0.06406         6       640: 100% 26/26 [00:11<00:00,  2.36it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.90it/s]\n",
            "                 all          95         144        0.61       0.467       0.489       0.327\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     64/99     9.26G     0.033   0.01726   0.01334    0.0636         4       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.87it/s]\n",
            "                 all          95         144        0.51       0.579        0.55       0.362\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     65/99     9.26G   0.03301   0.01662   0.01126    0.0609         4       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "                 all          95         144       0.577       0.614       0.569       0.361\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     66/99     9.26G    0.0317   0.01683   0.00995   0.05847         7       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.85it/s]\n",
            "                 all          95         144       0.635       0.677       0.683       0.453\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     67/99     9.26G   0.03046   0.01713   0.01175   0.05934         5       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "                 all          95         144       0.537       0.692       0.631       0.425\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     68/99     9.26G   0.03314   0.01541   0.01129   0.05984         1       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.82it/s]\n",
            "                 all          95         144       0.604       0.683       0.655       0.444\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     69/99     9.26G   0.03111    0.0168   0.01115   0.05906         7       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.90it/s]\n",
            "                 all          95         144       0.737       0.582       0.654       0.469\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     70/99     9.26G   0.02798   0.01553  0.008158   0.05166         8       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.93it/s]\n",
            "                 all          95         144       0.577       0.668       0.659       0.449\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     71/99     9.26G   0.02839   0.01428   0.01028   0.05296         4       640: 100% 26/26 [00:11<00:00,  2.36it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.88it/s]\n",
            "                 all          95         144       0.616       0.668       0.652       0.444\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     72/99     9.26G   0.02916   0.01496   0.00685   0.05098         7       640: 100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "                 all          95         144       0.687       0.594       0.604       0.415\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     73/99     9.26G   0.03173   0.01535  0.007625   0.05471         5       640: 100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.84it/s]\n",
            "                 all          95         144       0.654       0.673       0.649       0.433\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     74/99     9.26G   0.02851    0.0151  0.009354   0.05296         5       640: 100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.84it/s]\n",
            "                 all          95         144       0.692       0.678       0.644       0.443\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     75/99     9.26G   0.02581   0.01429  0.007105   0.04721         7       640: 100% 26/26 [00:11<00:00,  2.33it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.83it/s]\n",
            "                 all          95         144       0.671       0.683       0.645       0.436\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     76/99     9.26G   0.02484   0.01413  0.009186   0.04815         4       640: 100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.85it/s]\n",
            "                 all          95         144       0.654       0.681       0.655       0.446\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     77/99     9.26G   0.03028   0.01567  0.006738   0.05269        11       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.85it/s]\n",
            "                 all          95         144       0.647       0.614       0.607        0.42\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     78/99     9.26G    0.0242   0.01449  0.007939   0.04663         4       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.92it/s]\n",
            "                 all          95         144        0.65       0.582       0.594       0.424\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     79/99     9.26G   0.02754   0.01648  0.008156   0.05217         8       640: 100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.92it/s]\n",
            "                 all          95         144       0.647       0.592       0.592       0.436\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     80/99     9.26G   0.02725    0.0151  0.006558   0.04891         9       640: 100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.93it/s]\n",
            "                 all          95         144       0.648       0.696       0.658       0.471\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     81/99     9.26G   0.02594   0.01439  0.009598   0.04993         8       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.86it/s]\n",
            "                 all          95         144       0.705       0.578       0.623       0.439\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     82/99     9.26G   0.02489   0.01386   0.00811   0.04686         6       640: 100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.82it/s]\n",
            "                 all          95         144       0.648       0.548       0.576       0.409\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     83/99     9.26G   0.02761   0.01378  0.007193   0.04859         6       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.94it/s]\n",
            "                 all          95         144       0.659       0.612       0.604       0.442\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     84/99     9.26G   0.02869   0.01494  0.006853   0.05049        10       640: 100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.81it/s]\n",
            "                 all          95         144       0.642       0.632       0.621       0.443\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     85/99     9.26G   0.02621   0.01336  0.009128    0.0487         4       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.91it/s]\n",
            "                 all          95         144       0.714        0.62       0.628       0.455\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     86/99     9.26G    0.0261   0.01368    0.0055   0.04529         4       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.82it/s]\n",
            "                 all          95         144       0.721       0.673       0.652       0.475\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     87/99     9.26G   0.02112   0.01301   0.00861   0.04275         1       640: 100% 26/26 [00:11<00:00,  2.36it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.87it/s]\n",
            "                 all          95         144       0.727        0.63        0.64       0.462\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     88/99     9.26G   0.02337   0.01303   0.01042   0.04682         2       640: 100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.92it/s]\n",
            "                 all          95         144       0.623       0.698       0.645       0.463\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     89/99     9.26G   0.02384   0.01388  0.005808   0.04353         4       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "                 all          95         144       0.715       0.677       0.656       0.469\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     90/99     9.26G   0.02313   0.01422  0.006624   0.04397         6       640: 100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.92it/s]\n",
            "                 all          95         144       0.661        0.58       0.588       0.414\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     91/99     9.26G   0.02676    0.0137  0.006912   0.04737         7       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "                 all          95         144       0.739       0.578       0.593       0.412\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     92/99     9.26G   0.02338   0.01264  0.006001   0.04202         2       640: 100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.87it/s]\n",
            "                 all          95         144       0.739       0.632        0.64       0.463\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     93/99     9.26G   0.02472     0.013  0.006196   0.04391         8       640: 100% 26/26 [00:11<00:00,  2.36it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.86it/s]\n",
            "                 all          95         144       0.698        0.67        0.66       0.479\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     94/99     9.26G   0.02333   0.01261  0.004923   0.04086         7       640: 100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.81it/s]\n",
            "                 all          95         144       0.705       0.662       0.663       0.477\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     95/99     9.26G   0.02465   0.01351  0.008414   0.04657         6       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.87it/s]\n",
            "                 all          95         144       0.724       0.675       0.687       0.502\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     96/99     9.26G   0.02496    0.0122  0.006776   0.04394         4       640: 100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.87it/s]\n",
            "                 all          95         144       0.737        0.68       0.694       0.496\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     97/99     9.26G   0.02336   0.01307  0.007255   0.04369         3       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.92it/s]\n",
            "                 all          95         144       0.764       0.693       0.706       0.522\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     98/99     9.26G   0.02373   0.01335  0.005242   0.04232         5       640: 100% 26/26 [00:11<00:00,  2.34it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  2.83it/s]\n",
            "                 all          95         144       0.758       0.695       0.694       0.506\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     99/99     9.26G   0.02428    0.0132  0.006536   0.04402         4       640: 100% 26/26 [00:11<00:00,  2.35it/s]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 3/3 [00:01<00:00,  1.65it/s]\n",
            "                 all          95         144       0.731       0.652       0.683       0.503\n",
            "        curry leaves          95          18       0.887       0.873       0.957       0.643\n",
            "        garam masala          95          27       0.727       0.852       0.775       0.539\n",
            "                ghee          95          11       0.834       0.818       0.743       0.664\n",
            "              ginger          95          12       0.462         0.5       0.401       0.238\n",
            "             jaggery          95          14       0.577       0.391       0.399       0.299\n",
            "                milk          95          12        0.94       0.583       0.839       0.566\n",
            "          rice flour          95           9       0.755       0.556       0.671       0.557\n",
            "               sugar          95          11       0.548       0.364       0.495       0.368\n",
            "              tomato          95          24       0.777        0.58       0.601       0.374\n",
            "            urad dal          95           6       0.808           1       0.948       0.784\n",
            "Optimizer stripped from runs/train/food_ingredients_yolov5m/weights/last.pt, 42.6MB\n",
            "Optimizer stripped from runs/train/food_ingredients_yolov5m/weights/best.pt, 42.6MB\n",
            "100 epochs completed in 0.362 hours.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ3S0GzKlv2V"
      },
      "source": [
        "# Exporting to TorchScript & Onnx model\n",
        "\n",
        "### Requires a few changes in the export file copying methods related to inference for further use.Refer to [the endpoint repository](https://github.com/himanshu-dutta/mbc-endpoint)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSaVJGEvopx7"
      },
      "source": [
        "!pip install onnx>=1.8.0 -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "us_WI1aVlq6b",
        "outputId": "3e56a484-5a5c-4440-f1b5-0c72659bcaf1"
      },
      "source": [
        "!python models/export.py --weights runs/train/food_ingredients_yolov5m/weights/best.pt --img 640 --batch 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=1, img_size=[640, 640], weights='runs/train/food_ingredients_yolov5m/weights/best.pt')\n",
            "Fusing layers... \n",
            "Model Summary: 308 layers, 21074007 parameters, 0 gradients, 50.4 GFLOPS\n",
            "<class 'tuple'>\n",
            "2\n",
            "torch.Size([1, 25200, 15]) <class 'torch.Tensor'>\n",
            "\n",
            "Starting TorchScript export with torch 1.7.0+cu101...\n",
            "./models/yolo.py:53: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if check0_ or check1_:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/jit/_trace.py:940: TracerWarning: Encountering a list at the output of the tracer might cause the trace to be incorrect, this is only valid if the container structure does not change based on the module's inputs. Consider using a constant container instead (e.g. for `list`, use a `tuple` instead. for `dict`, use a `NamedTuple` instead). If you absolutely need this and know the side effects, pass strict=False to trace() to allow this behavior.\n",
            "  _force_outplace,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f8CZHEimn0p"
      },
      "source": [
        "# exporting the run to g-drive\n",
        "!cp -R /content/yolov5/runs/train/food_ingredients_yolov5m /content/drive/MyDrive/Projects/food-ingredients/top-10/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aubYEHOpv1BD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}